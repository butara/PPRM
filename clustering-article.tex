\documentclass[]{IEEEphot}
\usepackage[utf8]{inputenc}

\jvol{xx}
\jnum{xx}
\jmonth{November}
\pubyear{2011}

\begin{document}
\title{Machine Learning: Clustering algorithms review}
\author{Mentor, Primož, Matej}
\affil{Fakulteta za računalništvo in informatiko}  
\maketitle

\begin{abstract}
Three dimensional images were obtained using a single high numerical aperture hologram recorded in a high resolution photoresist with a table top $\alpha = 46.9$ nm laser. Gabor holograms numerically reconstructed over a range of image planes by sweeping the propagation distance allow numerical optical sectioning that results in a robust three dimension image of a test object with a resolution in depth of approximately and a lateral resolution of 164 nm. 
\end{abstract}

\begin{IEEEkeywords}
Machine learning, clustering, k-means, ECMC, Expectation Maximization.
\end{IEEEkeywords}

\section{Introduction}

What is clustering? It's an unsupervised machine learning task. Computationally difficult
(NP-hard). Heuristic algorithms. Iterative refinements.

At vero eos et accusamus et iusto odio dignissimos ducimus qui blanditiis praesentium voluptatum deleniti atque corrupti quos dolores et quas molestias excepturi sint occaecati cupiditate non provident, similique sunt in culpa qui officia deserunt mollitia animi, id est laborum et dolorum fuga. Et harum quidem rerum facilis est et expedita distinctio. Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil impedit quo minus id quod maxime placeat facere possimus, omnis voluptas assumenda est, omnis dolor repellendus. Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe eveniet ut et voluptates repudiandae sint et molestiae non recusandae. Itaque earum rerum hic tenetur a sapiente delectus, ut aut reiciendis voluptatibus maiores alias consequatur aut perferendis doloribus asperiores repellat.

\section{Related work}

At vero eos et accusamus et iusto odio dignissimos ducimus qui blanditiis praesentium voluptatum deleniti atque corrupti quos dolores et quas molestias excepturi sint occaecati cupiditate non provident, similique sunt in culpa qui officia deserunt mollitia animi, id est laborum et dolorum fuga. Et harum quidem rerum facilis est et expedita distinctio. Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil impedit quo minus id quod maxime placeat facere possimus, omnis voluptas assumenda est, omnis dolor repellendus. Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe eveniet ut et voluptates repudiandae sint et molestiae non recusandae. Itaque earum rerum hic tenetur a sapiente delectus, ut aut reiciendis voluptatibus maiores alias consequatur aut perferendis doloribus asperiores repellat.

\section{Algorithms}

\subsection{k-means}
k-means clustering (KMC) is a simple and well known algorithm. It is usually very
fast (commonly run multiple times with different starting conditions), but tends
to produce spherical clusters (with similar size), which is often undesirable.
k-means is often used as a preprocessing step for other algorithms. The standard
algorithm was first proposed by Stuart Lloyd in 1957 and published later in 1982.

The number of cluster we want to find in data is fixed a priori (k). We will call
the centroid m1, m2, ... mk. The algorithm iteratively refines
locations of clusters' centroids, until no more refinements
are possible or max number of iterations has been reached.
There are two alternating steps:

- assignment step: examples are assigned to closest clusters
  (we can define closest as smallest euclidean distance), that is: example xi
  is assigned to cluster Cl if dis(xi, ml) < dis(xj, mu) for u = 1...k, u != l

- update step: new means are calculated and set as centroids:

    ml = 1/N sum xki

	TODO

Although it can be proved that the procedure will always terminate (converge),
the k-means algorithm does not necessarily find the most optimal configuration,
corresponding to the global objective function minimum. The algorithm is also sensitive 
to the initial selected cluster centroids but it can be run multiple times to reduce this effect.

Another interesting fact is that, in the worst case, k-means can be very slow to converge:
in particular it has been shown that there exist certain point sets,
even in 2 dimensions, on which k-means takes exponential time, that is 2(n), to converge.
These point sets do not seem to arise in practice: this is corroborated by the
fact that the smoothed running time of k-means is polynomial.

\subsection{Evolving Clustering Method}

\subsection{Expectation Maximization Gaussian Mixture Model}

\subsection{Cauchy-Schwarz Divergence Clustering}

\section{Results}

\subsection{Method}
At vero eos et accusamus et iusto odio dignissimos ducimus qui blanditiis praesentium voluptatum deleniti atque corrupti quos dolores et quas molestias excepturi sint occaecati cupiditate non provident, similique sunt in culpa qui officia deserunt mollitia animi, id est laborum et dolorum fuga. Et harum quidem rerum facilis est et expedita distinctio. Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil impedit quo minus id quod maxime placeat facere possimus, omnis voluptas assumenda est, omnis dolor repellendus. Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe eveniet ut et voluptates repudiandae sint et molestiae non recusandae. Itaque earum rerum hic tenetur a sapiente delectus, ut aut reiciendis voluptatibus maiores alias consequatur aut perferendis doloribus asperiores repellat.

\subsection{Data preprocessing}

\section{Conclusions}

At vero eos et accusamus et iusto odio dignissimos ducimus qui blanditiis praesentium voluptatum deleniti atque corrupti quos dolores et quas molestias excepturi sint occaecati cupiditate non provident, similique sunt in culpa qui officia deserunt mollitia animi, id est laborum et dolorum fuga. Et harum quidem rerum facilis est et expedita distinctio. Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil impedit quo minus id quod maxime placeat facere possimus, omnis voluptas assumenda est, omnis dolor repellendus. Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe eveniet ut et voluptates repudiandae sint et molestiae non recusandae. Itaque earum rerum hic tenetur a sapiente delectus, ut aut reiciendis voluptatibus maiores alias consequatur aut perferendis doloribus asperiores repellat.

\section*{Acknowledgements}
Thanks to TODO.

%% \ackrule

%\bibliographystyle{IEEEtran}
%\bibliography{thesis}

\section*{References}

\end{document}
