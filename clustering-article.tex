\documentclass[]{IEEEphot}
\usepackage[utf8]{inputenc}
\usepackage[parfill]{parskip}

\jvol{xx}
\jnum{xx}
\jmonth{November}
\pubyear{2011}

\begin{document}
\title{Machine Learning: Clustering algorithms review}
\author{Mentor, Primož, Matej}
\affil{Fakulteta za računalništvo in informatiko}  
\maketitle

\begin{abstract}
Abstract is missing!
\end{abstract}

\begin{IEEEkeywords}
Machine learning, clustering, k-means, ECMC, Expectation Maximization.
\end{IEEEkeywords}

\section{Introduction}
What is clustering? It's an unsupervised machine learning task. Computationally difficult
(NP-hard). Heuristic algorithms. Iterative refinements.

\section{Related work}

\newpage
\section{Algorithms}
\subsection{k-means}
k-means clustering (KMC) is a simple and well known algorithm. It is usually very
fast (commonly run multiple times with different starting conditions), but tends
to produce spherical clusters with similar size, which can be undesirable.
k-means is often used as a preprocessing step for other algorithms. The standard
algorithm was first proposed by Stuart Lloyd in 1957 and published later in 1982.

The number of clusters we want to find in data is fixed a priori ($k$).
The algorithm iteratively refines clusters' centroids,
until no more refinements are possible (centroids do not move anymore)
or max number of iterations has been reached. $m_1^{(1)}$, $m_2^{(1)}$, ..., $m_k^{(1)}$
denote centroids (also called means). The superscript index denotes the current iteration.
$S_i^{(t)}$ is a cluster (set of examples) with the corresponding centroid $m_i^{(t)}$.
There are two alternating steps in the algorithm:

\begin{itemize}
\item \textbf{Assignment step}\\
  Each example is assigned to the cluster with the closest centroid:\\
  $S_i^{(t)} = \{x_j : ||x_j - m_i^{(t)}|| \le ||x_j - m_l^{(t)}|| \forall l = 1, ..., k\}$
\\
\item \textbf{Update step}\\
  New means are calculated and set as centroids:\\
  $m_i^{(t+1)} = \frac{1}{|S_i^{(t)}|} \displaystyle\sum\limits_{x_j \in S_i^{(t)}}{x_j}$
\end{itemize}

There are two common ways to select the initial centroids: random seed and random partition.
The random seed method randomly chooses k observations from the data set
and uses these as the initial means. Random partition
assigns each example into one of the k clusters randomly.
The random seed method tends to spread the initial means out, while random partition
places all of them close to the center of the data set.

There are also better approaches to initialization: k-means++ algorithm specifies a
procedure to initialize the cluster centers before proceeding with the standard k-means
optimization iterations. With the k-means++ initialization, the algorithm is guaranteed
to find a solution that is $O(\log k)$ competitive to the optimal k-means solution.
Although this initial selection takes extra time, it speeds up the convergence and 
therefore actually lowers overall computation time.

Although it can be proved that the KMC algorithm will always terminate (converge),
it does not necessarily find the most optimal configuration,
corresponding to the global objective function minimum. The algorithm is also sensitive
to the initial selected cluster centroids but it can be run multiple times to reduce this effect.

Another interesting fact is that, in the worst case, k-means can be very slow to converge:
in particular it has been shown that there exist certain point sets,
even in 2 dimensions, on which k-means takes exponential time to converge.
These point sets do not seem to arise in practice: this is supported by the
fact that the smoothed time complexity of k-means is polynomial.

\subsection{Evolving Clustering Method}

ECM (Evolving Clustering Method) is dynamic clustering method mostly used for on-line system with fast "one-pass" algorithm. It's extension ECMc (Evolving Clustering Method with Constrained minimization) is used for off-line where constrained optimisation is applied. Both ECM and it's extension were introduced in 2001 by Qun Song and Nikola Kasabov.

ECM is a distance based clustering method where each cluster has a centre, where all of cluster's members are less distant from the centre than $D_{thr}$. $D_{thr}$ is by default input in algorithm, unlike to some others where number of cluster is input. However, $D_{thr}$ does affect number of clusters but not directly. Therefore, when comparing ECM algorithm to others you have to be aware of their inputs.

ECM takes samples from a data stream and with every new data cluster's radius can be updated or it's center changed or new cluster can be created. At some point, usually when cluster radius reaches threshold value $D_{thr}$, cluster radius will not update anymore.

Both ECM and ECMc consist of several steps where ECM is a pre-process to ECMc. First sample is used to create first cluster with radius 0 and centre on the sample itself. For every new sample $x_i$ next steps are taken:
\begin{enumerate}
\item Distance from $x_i$ to every cluster centre is calculated as shown in equation \ref{ECMdist}.
\item If minimum distance found is less than or equal to cluster radius, $x_i$ belongs to cluster and algorithm returns to step 1.
\item For every cluster s is calculated as shown in equation \ref{ECMequ1}. If minimum s is more than $2*D_{thr}$ a new cluster is created and algorithm continues at step 1.
\item Cluster's centre, with minumum s calculated, is moved and radius increased. New radius is calculated as $s/2$ and centre's position is on the line from old centre to sample $x_i$ so that distance from new centre to point $x_i$ is equal to $s/2$ as shown in equation \ref{ECMequ2} and \ref{ECMequ3}.
\end{enumerate}

\begin{equation}\label{ECMdist}
dist(i,j) = \sqrt{ \frac {\sum_{d=1}^{D} (x_i^d - C_{cj}^d)^2} {D}}
\end{equation}

\begin{equation}\label{ECMequ1}
s(i, j) = dist(i,j) + radius(C_j)
\end{equation}

\begin{equation}\label{ECMequ2}
f = 1 - \frac {\frac {s} {2}} {\sqrt{ \sum_{d=1}^{D} (C_c^d - x_i^d)}}
\end{equation}
\begin{equation}\label{ECMequ3}
Cc^d = Cc^d + (f * (x_i^d - C_c^d))
\end{equation}

This way it holds for each sample in its own cluster that the distance between centre and sample is less than or equal to $D_{thr}$. Afterwards constrained optimisation can be applied to minimize the function \ref{ECMJ}. For optimisation the following steps have to be taken:
\begin{enumerate}
\item 1. Change the membership of each sample in data to cluster that has the smallest distance to it's centre
\item 2. Calculate new cluster centre to fit new changes
\item 3. The end of algorithm is reached if enough iterations occured or the change of function \ref{ECMJ} is less than certain amount. Otherwise continue with going back to step 1.
\end{enumerate}

\begin{equation}\label{ECMJ}
J = \sum_{k=1}^K \sum_{i=1}^{N_{Ck}} dist(x_{ki}, C_{ck})
\end{equation}

\subsection{Expectation Maximization Gaussian Mixture Model}

\subsection{Cauchy-Schwarz Divergence Clustering}
KMC is an example of so-called \textit{parametric} methods of clustering. These
methods assume some knowledge about clusters' structure, which makes them less
suitable for many use cases. KMC performs badly when clusters are not hyperelliptical
because it implicitly assumes Gaussian cluster distributions. Any clustering method
that utilizes second order data statistics can produce only convex clusters (Jain et al., 2000).

Information theory has been successfully used in clustering by several researchers in recent years.
Various information-theoretic clustering metrics, such as entropy, mutual information and
Kullback-Leibler divergence were researched.

\section{Results}
\subsection{Method}
\subsection{Data preprocessing}

\section{Conclusions}

\section*{Acknowledgements}
Thanks to TODO.

\section*{References}

\end{document}